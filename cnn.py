{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Reference - https://github.com/isegura/MultimodalFakeNewsDetection\n\n\n# Import libraries\nimport numpy as np # version - 1.21.6\nimport time\nimport matplotlib # version - 3.5.3\nimport matplotlib.image as mpimg\nimport pandas as pd # version - 1.3.5\nfrom torch import optim # version - 1.11.0\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer # sklearn version - 1.0.2\nimport nltk # version - 3.7\nnltk.download('wordnet')\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re # version - 2.2.1\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.preprocessing.text import Tokenizer # tensorflow version - 2.6.4\nimport spacy # version - 3.3.1\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import backend as K\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n%matplotlib inline\n\n# Train data (with and without images)\ntraindata_all = pd.read_csv(\"./all_train.tsv\", sep='\\t') # path to be changed based on the path of data file\n# Validation data (with and without images)\nvalidata_all = pd.read_csv(\"./all_validate.tsv\", sep='\\t') # path to be changed based on the path of data file\n# Test data (with and without images)\ntestdata_all = pd.read_csv(\"./all_test_public.tsv\", sep='\\t') # path to be changed based on the path of data file\n\n# cleaning the training data\ntrain_data_all = traindata_all[traindata_all['clean_title'].notnull().to_numpy()]\n# cleaning the validation data\nvalid_data_all = validata_all[validata_all['clean_title'].notnull().to_numpy()]\n# cleaning the testing data\ntest_data_all = testdata_all[testdata_all['clean_title'].notnull().to_numpy()]\n\n# separating the data and labels\ntrain_frame = train_data_all[\"clean_title\"]\ntrain_labels = train_data_all[\"2_way_label\"]\n\nvalid_frame = valid_data_all[\"clean_title\"]\nvalid_labels = valid_data_all[\"2_way_label\"]\n\ntest_frame = test_data_all[\"clean_title\"]\ntest_labels = test_data_all[\"2_way_label\"]\n\n# first we remove punctuations and numbers and also multiple spaces\n\ntrain_list = list(train_frame)\nvalid_list = list(valid_frame)\ntest_list = list(test_frame)\n\ntrain_labels_list = list(train_labels)\nvalid_labels_list = list(valid_labels)\ntest_labels_list = list(test_labels)\n\n\ndef preprocess_text(sen):\n    # Remove punctuations and numbers\n    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n\n    return sentence\n\n\ntrain_news_clean_1 = []\nvalid_news_clean_1 = []\ntest_news_clean_1 = []\n\nfor new in train_list:\n    train_news_clean_1.append(preprocess_text(new))\n\nfor new in valid_list:\n    valid_news_clean_1.append(preprocess_text(new))\n\nfor new in test_list:\n    test_news_clean_1.append(preprocess_text(new))\n    \n\n# Function to remove stop words and perform lemmatization\n# Initialize lemmatizer\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nlemmatizer = WordNetLemmatizer()\n\n# Initialize stemmer and stop_words\n#stemmer = PorterStemmer()\nstop_words = set(stopwords.words('english')) \n\n\n# Function to remove stopwords\ndef remove_stopwords_lem(text):\n    text = word_tokenize(text)\n    # Stop words removal\n    text = [word for word in text if word not in stop_words]\n    # Lematization\n    lemmatized_text = []\n    for word in text:\n        word1 = lemmatizer.lemmatize(word, pos = \"n\")\n        word2 = lemmatizer.lemmatize(word1, pos = \"v\")\n        word3 = lemmatizer.lemmatize(word2, pos = (\"a\"))\n        lemmatized_text.append(word3) \n    text_done = ' '.join(lemmatized_text)\n    return text_done\n\n\n# Stop-words removal and lemmatization\ntrain_stwrd_lem = []\nvalid_stwrd_lem = []\ntest_stwrd_lem = []\n\nfor new in train_news_clean_1:\n    train_stwrd_lem.append(remove_stopwords_lem(new))\n\nfor new in valid_news_clean_1:\n    valid_stwrd_lem.append(remove_stopwords_lem(new))\n\nfor new in test_news_clean_1:\n    test_stwrd_lem.append(remove_stopwords_lem(new))\n\nnews_all = train_stwrd_lem + valid_stwrd_lem + test_stwrd_lem\n\ntokenizer = Tokenizer(num_words = 128022)\ntokenizer.fit_on_texts(news_all)\n\n# Tokenize news\n\ntrain_tokenized = tokenizer.texts_to_sequences(train_stwrd_lem)\nvalid_tokenized = tokenizer.texts_to_sequences(valid_stwrd_lem)\ntest_tokenized = tokenizer.texts_to_sequences(test_stwrd_lem)\n\n\n# Function to count the lenght of each sequence\ndef length_squences(data):\n    lengths = []\n    for i in range(len(data)):\n        lengths.append(len(data[i]))\n    return lengths\n\n\n# Pad/truncate the tokenized news\n\ntrain_tokenized_pad = pad_sequences(train_tokenized, maxlen = 128, truncating = 'pre', padding = 'pre')\nvalid_tokenized_pad = pad_sequences(valid_tokenized, maxlen = 128, truncating = 'pre', padding = 'pre')\ntest_tokenized_pad = pad_sequences(test_tokenized, maxlen = 128, truncating = 'pre', padding = 'pre')\n\n# Transform data arrays into tensors\n\ntrain_tensor = torch.Tensor(train_tokenized_pad).int()\nvalid_tensor = torch.Tensor(valid_tokenized_pad).int()\ntest_tensor =  torch.Tensor(test_tokenized_pad).int()\n\n# Tranform tensors into data loader objects\n\ntrain_set = TensorDataset(train_tensor, torch.Tensor(np.array(train_labels)))\ntrainloader = DataLoader(train_set, batch_size=64)\n\nvalid_set = TensorDataset(valid_tensor, torch.Tensor(np.array(valid_labels)))\nvalidloader = DataLoader(valid_set, batch_size=64)\n\ntest_set = TensorDataset(test_tensor, torch.Tensor(np.array(test_labels)))\ntestloader = DataLoader(test_set, batch_size=64)\n\n\n# Function to load the word embeddings\n\ndef load_embedd(filename):\n    words = []\n    vectors = []\n    file = open(filename,'r', encoding=\"utf8\")\n    for line in file.readlines():\n       row = line.split(' ')\n       vocab = row[0]\n       embd = row[1:len(row)]\n       embd[-1] = embd[-1].rstrip()\n       embd = list(map(float,embd)) # convert string to float\n       words.append(vocab)\n       vectors.append(embd)\n    file.close()\n    return words,vectors\n\n\n# Function to create the embedding matrix\n\ndef embed_matx(word_index, vocab, embeddings, length_vocab, length_embedding):\n    embedding_matrix = np.zeros((length_vocab +1, length_embedding))\n    for word, i in word_index.items():\n        if word in vocab:\n            idx = vocab.index(word)\n            vector =  embeddings[idx]\n            embedding_matrix[i] = vector\n        if i == length_vocab:\n            break\n    return embedding_matrix\n\n\nvocab_gv_300, vectors_gv_300 = load_embedd(filename = \"./glove-embeddings/glove.6B.300d.txt\") # path to be changed based on the path of data file\n\nword_index = tokenizer.word_index\n# Embedding matrix\nembedding_matrix_gv_300 = embed_matx(word_index = word_index, vocab = vocab_gv_300, embeddings = vectors_gv_300, \n                             length_vocab = 142720, length_embedding = 300)\n\n\n# Model\nclass CNN(nn.Module):\n    def __init__(self, nlabels, train_parameters = True, random_embeddings = True): \n        super().__init__()\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(num_embeddings = 142720, embedding_dim = 300)\n        \n        if random_embeddings == True:\n            self.embedding.weight = nn.Parameter(torch.rand(142720, 300), requires_grad = train_parameters)\n        else:\n            self.embedding.weight = nn.Parameter(torch.from_numpy(embedding_matrix_gv_300), requires_grad = train_parameters)\n            \n        # Filters for the CNN    \n        self.filter_sizes = [2,3,4,5]\n        self.num_filters = 50\n        \n        # Concolutional layers\n        self.convs_concat = nn.ModuleList([nn.Conv2d(1, self.num_filters, (K, 300)) for K in self.filter_sizes])\n        \n        # Linear layers\n        self.linear1 = nn.Linear(200,128)\n  \n        self.linear2 = nn.Linear(128,nlabels)\n    \n        self.relu = nn.ReLU()\n        \n        self.logsoftmax = nn.LogSoftmax(dim=1) \n        \n        \n    def forward(self, x):\n        # Embedding\n        x = self.embedding(x)\n        # Unsqueeze\n        x = x.unsqueeze(1)\n        # Convolution\n        x = [F.relu(conv(x.float())).squeeze(3) for conv in self.convs_concat]\n        # Max-pooling\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] \n        x = torch.cat(x, 1)\n        # Linear layers\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.linear2(x)\n        x = self.logsoftmax(x) \n        return x\n\n\n# Function for training\nclass CNN_extended(CNN):\n    \n    def __init__(self,nlabels, train_parameters, random_embeddings, epochs=100, lr=0.001):\n        \n        super().__init__(nlabels, train_parameters, random_embeddings)  \n        \n        self.lr = lr # Learning Rate\n        \n        self.optim = optim.Adam(self.parameters(), self.lr)\n        \n        self.epochs = epochs\n        \n        self.criterion = nn.NLLLoss()              \n        \n        # A list to store the loss evolution along training\n        \n        self.loss_during_training = [] \n\n        self.valid_loss_during_training = []\n        \n    def trainloop(self,trainloader, validloader):\n        \n        \n        # Optimization Loop\n        \n        for e in range(int(self.epochs)):\n\n            start_time = time.time()\n            \n            # Random data permutation at each epoch\n            \n            running_loss = 0.\n            \n            i = 0\n            \n            length = 0\n            \n            accuracies = []\n            \n            for news, labels in trainloader:             \n        \n                self.optim.zero_grad()  # Reset gradients\n            \n                out = self.forward(news.int())\n\n                loss = self.criterion(out,labels.long())\n                \n                loss.backward()\n\n                running_loss += loss.item()\n\n                self.optim.step()\n                \n                top_p, top_class = out.topk(1, dim=1)\n                \n                equals = (top_class == labels.view(news.shape[0], 1))\n                \n                length += news.shape[0]\n                \n                accuracies.append(sum(equals)) \n                \n                accuracy = sum(accuracies)/length\n                \n                i += 1\n                \n                if i%1000 == 0:\n                    print(\" Train accuracy: \", accuracy)\n                \n            self.loss_during_training.append(running_loss/len(trainloader))\n\n            # Validation Loss\n            \n            with torch.no_grad():            \n                \n                running_loss = 0.\n                \n                i = 0\n                \n                length = 0\n                \n                accuracies = []\n                \n                for news,labels in validloader:\n                    \n                    out = self.forward(news.int())\n\n                    loss = self.criterion(out,labels.long())\n\n                    running_loss += loss.item()   \n                    \n                    top_p, top_class = out.topk(1, dim=1)\n                \n                    equals = (top_class == labels.view(news.shape[0], 1))\n                \n                    length += news.shape[0]\n                \n                    accuracies.append(sum(equals)) \n                \n                    accuracy = sum(accuracies)/length\n                    \n                print(\" Validation accuracy: \", accuracy)                    \n                      \n                self.valid_loss_during_training.append(running_loss/len(validloader))\n\n            if(e % 1 == 0): # Every 10 epochs\n\n                print(\"Training loss after %d epochs: %f\" \n                      %(e,self.loss_during_training[-1]), \"Validation loss after %d epochs: %f\" %(e,self.valid_loss_during_training[-1]),\n                      \"Time per epoch: %f seconds\"%(time.time() - start_time))\n\n\n# Initialize model\nCNN_train_not_random = CNN_extended(nlabels = 2, epochs = 100, lr = 0.003, train_parameters = True, random_embeddings = False)\n# Train model\nCNN_train_not_random.trainloop(trainloader, validloader)\n\n# We join the train and validation datasets and we train the model with this dataset using the optimal number of epochs. \n# Then we evaluate the model with the test set. First we need to modify a bit the class used previously.\n\n# Join train and validation sequences\ntrain_valid_tokenized_pad = np.concatenate((train_tokenized_pad, valid_tokenized_pad), axis = 0)\n# Join train and validation labels\ntrain_valid_labels = np.concatenate((np.array(train_labels), np.array(valid_labels)), axis = 0)\n\n# Create tensor objects\n\ntrain_valid_tensor = torch.Tensor(train_valid_tokenized_pad).int()\n\ntest_tensor =  torch.Tensor(test_tokenized_pad).int()\n\n# Tranform tensors into data loader objects\n\ntrain_valid_set = TensorDataset(train_valid_tensor, torch.Tensor(np.array(train_valid_labels)))\ntrain_valid_loader = DataLoader(train_valid_set, batch_size=60)\n\ntest_set = TensorDataset(test_tensor, torch.Tensor(np.array(test_labels)))\ntestloader =  DataLoader(test_set, batch_size=60)\n\n# Initialize model\nCNN_test_train_not_random = CNN_extended(nlabels = 2, epochs = 100, lr = 0.003, train_parameters = True, random_embeddings = False)\n# Train model\nCNN_test_train_not_random.trainloop(train_valid_loader)\n# Get predictions\npredictions_2 = CNN_test_train_not_random.eval_performance(testloader)\n\nprint(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_2))\n\n# Confusion matrix\nprint(confusion_matrix(np.array(test_labels).reshape(len(test_labels),1), predictions_2))\n\n\n\n     \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}