{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Reference - https://github.com/isegura/MultimodalFakeNewsDetection\n\n\n# Import libraries\nimport numpy as np # version - 1.21.6\nimport optuna # version - 3.0.3\nimport matplotlib # version - 3.5.3\nimport matplotlib.image as mpimg\nimport pandas as pd # version - 1.3.5\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer # sklearn version - 1.0.2\nimport nltk # version - 3.7\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re # version - 2.2.1\nfrom tensorflow.keras.preprocessing.text import Tokenizer # tensorflow version - 2.6.4\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report\n\n%matplotlib inline\n\n# Train data (with and without images)\ntraindata_all = pd.read_csv(\"./all_train.tsv\", sep='\\t') # path to be changed based on the path of data file\n# Validation data (with and without images)\nvalidata_all = pd.read_csv(\"./all_validate.tsv\", sep='\\t') # path to be changed based on the path of data file\n# Test data (with and without images)\ntestdata_all = pd.read_csv(\"./all_test_public.tsv\", sep='\\t') # path to be changed based on the path of data file\n\n# cleaning the training data\ntrain_data_all = traindata_all[traindata_all['clean_title'].notnull().to_numpy()]\n# cleaning the validation data\nvalid_data_all = validata_all[validata_all['clean_title'].notnull().to_numpy()]\n# cleaning the testing data\ntest_data_all = testdata_all[testdata_all['clean_title'].notnull().to_numpy()]\n\n# separating the data and labels\ntrain_frame = train_data_all[\"clean_title\"]\ntrain_labels = train_data_all[\"2_way_label\"]\n\nvalid_frame = valid_data_all[\"clean_title\"]\nvalid_labels = valid_data_all[\"2_way_label\"]\n\ntest_frame = test_data_all[\"clean_title\"]\ntest_labels = test_data_all[\"2_way_label\"]\n\n# first we remove punctuations and numbers and also multiple spaces\n\ntrain_list = list(train_frame)\nvalid_list = list(valid_frame)\ntest_list = list(test_frame)\n\ntrain_labels_list = list(train_labels)\nvalid_labels_list = list(valid_labels)\ntest_labels_list = list(test_labels)\n\n\ndef preprocess_text(sen):\n    # Remove punctuations and numbers\n    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n\n    return sentence\n\n\ntrain_news_clean_1 = []\nvalid_news_clean_1 = []\ntest_news_clean_1 = []\n\nfor new in train_list:\n    train_news_clean_1.append(preprocess_text(new))\n\nfor new in valid_list:\n    valid_news_clean_1.append(preprocess_text(new))\n\nfor new in test_list:\n    test_news_clean_1.append(preprocess_text(new))\n\n\n# Function to remove stop words and perform lemmatization\n# Initialize lemmatizer\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nlemmatizer = WordNetLemmatizer()\n\n# Initialize stemmer and stop_words\n#stemmer = PorterStemmer()\nstop_words = set(stopwords.words('english')) \n\n\n# Function to remove stopwords\ndef remove_stopwords_lem(text):\n    text = word_tokenize(text)\n    # Stop words removal\n    text = [word for word in text if word not in stop_words]\n    # Lematization\n    lemmatized_text = []\n    for word in text:\n        word1 = lemmatizer.lemmatize(word, pos = \"n\")\n        word2 = lemmatizer.lemmatize(word1, pos = \"v\")\n        word3 = lemmatizer.lemmatize(word2, pos = (\"a\"))\n        lemmatized_text.append(word3) \n    text_done = ' '.join(lemmatized_text)\n    return text_done\n\n\ntrain_lemmatized = [remove_stopwords_lem(text) for text in train_news_clean_1]\n\nvalid_lemmatized = [remove_stopwords_lem(text) for text in valid_news_clean_1]\n\ntest_lemmatized = [remove_stopwords_lem(text) for text in test_news_clean_1]\n\n\n# Finding best hyper-parameters for multinomial naive bayes\ndef objective_Bayes(trial):\n    \n    # Sample values for the hyper-parameters\n    n = trial.suggest_int(\"n\", 1, 2)\n    sub_tf = trial.suggest_categorical(\"sub_tf\", [\"True\", \"False\"])\n    min_df = trial.suggest_int(\"min_df\",5,25)\n    # Create pipeline\n    Bayes_pipe = Pipeline([('vect', CountVectorizer(ngram_range = (1, n), min_df = min_df)),\n                            ('tfidf', TfidfTransformer(sublinear_tf = sub_tf)),('classifier', MultinomialNB() )])\n    # Fit model\n    clf_Bayes = Bayes_pipe.fit(train_lemmatized, train_labels_list)\n    # Obtain the predictions\n    predictions = Bayes_pipe.predict(valid_lemmatized)\n    # Obtain the accuracy\n    acc = accuracy_score(valid_labels_list, predictions)\n    \n    return acc\n\n\n# Select budget and set seed                            \nbudget = 40\nnp.random.seed(0)\n# Optimize hyper-parameters\nstudy_Bayes = optuna.create_study(direction=\"maximize\")\nstudy_Bayes.optimize(objective_Bayes, n_trials=budget,show_progress_bar=False)\n\n# Best hyper-parameters\nprint(\"Best hyper-parameters: \")\nprint(study_Bayes.best_params)\n# Best score\nprint(\"Best score: \")\nprint(study_Bayes.best_value)\n\n\n# Finding best hyper-parameters for logistic regression\ndef objective_Logistic(trial):\n    \n    # Sample values for the hyper-parameters\n    max_iter = trial.suggest_int(\"max_iter\", 320, 420)\n    solver = trial.suggest_categorical(\"solver\", [\"newton-cg\"])\n    multi_class = trial.suggest_categorical(\"multi_class\",[\"ovr\", \"multinomial\"])\n    n = trial.suggest_int(\"n\", 1, 2)\n    min_df = trial.suggest_int(\"min_df\",5,25)\n    sub_tf = trial.suggest_categorical(\"sub_tf\", [\"True\", \"False\"])\n    # Create pipeline\n    Logistic_pipe = Pipeline([('vect', CountVectorizer(ngram_range = (1, n), min_df = min_df)),\n                            ('tfidf', TfidfTransformer(sublinear_tf = sub_tf)),('classifier', LogisticRegression(random_state = 3,\n                                    solver = solver, multi_class = multi_class,   max_iter = max_iter ) )])\n\n    # Fit model\n    clf_Logistic = Logistic_pipe.fit(train_lemmatized, train_labels_list)\n    # Obtain the predictions\n    predictions = Logistic_pipe.predict(valid_lemmatized)\n    # Obtain the accuracy\n    acc = accuracy_score(valid_labels_list, predictions)\n    \n    return acc\n\n\n# Select budget and set seed                            \nbudget = 40\nnp.random.seed(0)\n# Optimize hyper-parameters\nstudy_Logistic = optuna.create_study(direction=\"maximize\")\nstudy_Logistic.optimize(objective_Logistic, n_trials=budget,show_progress_bar=False)\n\n# Best hyper-parameters\nprint(\"Best hyper-parameters: \")\nprint(study_Logistic.best_params)\n# Best score\nprint(\"Best score: \")\nprint(study_Logistic.best_value)\n\n\n# Finding best hyper-parameters for random forests\ndef objective_Forest(trial):\n    \n    # Sample values for the hyper-parameters\n    n_estimators = trial.suggest_int(\"n_estimators\", 100, 300)\n    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n    max_depth = trial.suggest_int(\"max_depth\", 3, 6)\n    n = trial.suggest_int(\"n\", 1, 2)\n    min_df = trial.suggest_int(\"min_df\",5,25)\n    sub_tf = trial.suggest_categorical(\"sub_tf\", [\"True\", \"False\"])\n    # Create pipeline\n    Forest_pipe = Pipeline([('vect', CountVectorizer(ngram_range = (1, n), min_df = min_df)),\n                            ('tfidf', TfidfTransformer(sublinear_tf = sub_tf)),('classifier', RandomForestClassifier(\n                                random_state = 3, n_estimators = n_estimators, criterion = criterion,\n                                max_depth = max_depth ) )])\n\n    # Fit model\n    clf_Forest = Forest_pipe.fit(train_lemmatized, train_labels_list)\n    # Obtain the predictions\n    predictions = Forest_pipe.predict(valid_lemmatized)\n    # Obtain the accuracy\n    acc = accuracy_score(valid_labels_list, predictions)\n    \n    return acc\n\n\n# Select budget and set seed                            \nbudget = 40\nnp.random.seed(0)\n# Optimize hyper-parameters\nstudy_Forest = optuna.create_study(direction=\"maximize\")\nstudy_Forest.optimize(objective_Forest, n_trials=budget,show_progress_bar=False)\n\n# Best hyper-parameters\nprint(\"Best hyper-parameters: \")\nprint(study_Forest.best_params)\n# Best score\nprint(\"Best score: \")\nprint(study_Forest.best_value)\n\n\n# Finding best hyper-parameters for linear SVM\ndef objective_SVC(trial):\n    \n    # Sample values for the hyper-parameters\n    max_iter = trial.suggest_int(\"max_iter\", 1000, 3000)\n    loss = trial.suggest_categorical(\"loss\", [\"hinge\", \"squared_hinge\"])\n    n = trial.suggest_int(\"n\", 1, 2)\n    min_df = trial.suggest_int(\"min_df\",5,25)\n    sub_tf = trial.suggest_categorical(\"sub_tf\", [\"True\", \"False\"])\n    # Create pipeline\n    SVC_pipe = Pipeline([('vect', CountVectorizer(ngram_range = (1, n), min_df = min_df)),\n                            ('tfidf', TfidfTransformer(sublinear_tf = sub_tf)),('classifier', LinearSVC(\n                                random_state = 3, max_iter = max_iter,\n                                loss = loss) )])\n\n    # Fit model\n    clf_SVC = SVC_pipe.fit(train_lemmatized, train_labels_list)\n    # Obtain the predictions\n    predictions = SVC_pipe.predict(valid_lemmatized)\n    # Obtain the accuracy\n    acc = accuracy_score(valid_labels_list, predictions)\n    \n    return acc\n\n\n# Select budget and set seed                            \nbudget = 40\nnp.random.seed(0)\n# Optimize hyper-parameters\nstudy_SVC = optuna.create_study(direction=\"maximize\")\nstudy_SVC.optimize(objective_SVC, n_trials=budget,show_progress_bar=False)\n\n# Best hyper-parameters\nprint(\"Best hyper-parameters: \")\nprint(study_SVC.best_params)\n# Best score\nprint(\"Best score: \")\nprint(study_SVC.best_value)\n\n\n# Join train and validation sets\ntraining_lemmatized = train_lemmatized + valid_lemmatized\n\n# Joining train and validation labels\ntraining_labels = train_labels_list + valid_labels_list\n\n\n# Training and testing the multinomial naive bayes classifier\nBayes_pipe_lem = Pipeline([('vect', CountVectorizer(ngram_range = (1, 2), min_df = 5)),\n                            ('tfidf', TfidfTransformer(sublinear_tf = 'True')),('classifier', MultinomialNB() )])\nBayes_pipe_lem.fit(training_lemmatized, training_labels)\n\n# Evaluation of the model\npredictions_Bayes_lem = Bayes_pipe_lem.predict(test_lemmatized)\nprint(classification_report(np.array(test_labels).reshape(len(test_labels),1),predictions_Bayes_lem))\n\n# Confusion matrix\nprint(confusion_matrix(np.array(test_labels).reshape(len(test_labels),1),predictions_Bayes_lem))\n\n\n# Training and testing logistic regression\nLogistic_pipe_lem = Pipeline([('vect', CountVectorizer(ngram_range = (1, 2), min_df = 5)),\n                            ('tfidf', TfidfTransformer(sublinear_tf = 'True')),('classifier', LogisticRegression(random_state = 3,\n                                    solver = 'newton-cg', multi_class = 'multinomial',   max_iter = 337 ) )])\nLogistic_pipe_lem.fit(training_lemmatized, training_labels)\n\n# Evaluation of the model\npredictions_Logistic_lem = Logistic_pipe_lem.predict(test_lemmatized)\nprint(classification_report(np.array(test_labels).reshape(len(test_labels),1),predictions_Logistic_lem))\n\n# Confusion matrix\nprint(confusion_matrix(np.array(test_labels).reshape(len(test_labels),1),predictions_Logistic_lem))\n\n\n# Training and testing random forests\nForest_pipe_lem = Pipeline([('vect', CountVectorizer(ngram_range = (1, 2), min_df = 9)),\n                            ('tfidf', TfidfTransformer(sublinear_tf = 'True')),('classifier', RandomForestClassifier(\n                                random_state = 3, n_estimators = 290, criterion = 'entropy',\n                                max_depth = 6) )])\nForest_pipe_lem.fit(training_lemmatized, training_labels)\n\n# Evaluation of the model\npredictions_Forest_lem = Forest_pipe_lem.predict(test_lemmatized)\nprint(classification_report(np.array(test_labels).reshape(len(test_labels),1),predictions_Forest_lem))\n\n# Confusion matrix\nprint(confusion_matrix(np.array(test_labels).reshape(len(test_labels),1),predictions_Forest_lem))\n\n\n# Training and testing linear SVM\nSVC_pipe_lem = Pipeline([('vect', CountVectorizer(ngram_range = (1, 2), min_df = 5)),\n                            ('tfidf', TfidfTransformer(sublinear_tf = 'True')),('classifier', LinearSVC(\n                                random_state = 3, max_iter = 1214,\n                                loss = 'squared_hinge') )])\nSVC_pipe_lem.fit(training_lemmatized, training_labels)\n\n# Evaluation of the model\npredictions_SVC_lem = SVC_pipe_lem.predict(test_lemmatized)\n\n# Classification report\nprint(classification_report(np.array(test_labels).reshape(len(test_labels),1),predictions_SVC_lem))\n\n# Confusion matrix\nprint(confusion_matrix(np.array(test_labels).reshape(len(test_labels),1),predictions_SVC_lem))\n\n\n\n\n\n\n","metadata":{"_uuid":"bce0dc78-68de-4094-abad-c2bd9ec377d5","_cell_guid":"7eb4564e-0c48-4baf-ac5a-496f0cc933b4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}